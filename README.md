# RAG-based Document Chatbot

This project is a Retrieval-Augmented Generation (RAG) based chatbot that allows you to upload a PDF and interact with it conversationally. The system retrieves semantically relevant document chunks using ChromaDB and generates contextual answers using a local LLM via Ollama.

---

## ğŸš€ Features

- Upload and chat with your own PDF documents
- Semantic document search using ChromaDB
- Fast and lightweight local LLM response via Ollama
- Conversational chat interface built with Streamlit
- Persistent vector store with SentenceTransformer embeddings

---

## ğŸ—‚ï¸ Project Structure

```
Rag-chat/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ interface.py          # Streamlit UI interface
â”‚   â”œâ”€â”€ retriever.py          # ChromaDB-based document retrieval logic
â”‚   â””â”€â”€ utils.py              # PDF processing utilities
â”œâ”€â”€ uploaded_docs/            # Folder to store uploaded PDFs
â”œâ”€â”€ chroma_db/                # Persistent Chroma vector storage
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ .gitignore                # Ignored files and folders
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # Project overview
```

---

### ğŸ› ï¸ Project Execution Steps

1. **Clone the Repository**

```bash
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot
```

2. **ğŸ“¦ Environment Setup**
   - Create a new Python environment using Anaconda or `venv`.
   - Install required dependencies using `pip`:
     ```bash
     pip install -r requirements.txt
     ```

3. **ğŸš€ Start Ollama**
   - Make sure [Ollama](https://ollama.com) is installed.
   - Pull and start the model (e.g., Llama 3):
     ```bash
     ollama pull llama3
     ollama serve
     ```
     
4. **ğŸ“ Directory Setup**
   Ensure the following structure is present:
   ```
   project/
   â”œâ”€â”€ app/
   â”‚   â”œâ”€â”€ interface.py
   â”‚   â””â”€â”€ retriever.py
   â”œâ”€â”€ utils.py
   â”œâ”€â”€ chat_histories/
   â”œâ”€â”€ requirements.txt
   ```

5. **ğŸ’¬ Run the Streamlit App**
   - From the project root, launch the app:
     ```bash
     streamlit run app/interface.py
     ```

6. **ğŸ“„ Upload Document & Chat**
   - Use the web interface to upload a PDF.
   - Ask questions about the content and receive structured responses.

7. **ğŸ’¾ Save & Rename Chats**
   - Use the sidebar to rename or delete chat sessions.
   - Chats are saved as `.json` files under the `chat_histories/` directory.

8. **ğŸ“Œ Notes**
   - Ensure Ollama is running before starting the app.
   - If you face module import issues, run Streamlit from the **project root**:
     ```bash
     cd project
     streamlit run app/interface.py
     ```
---

## âš™ï¸ How It Works

1. Upload a PDF document.
2. The file is split into text chunks.
3. Each chunk is embedded and stored in ChromaDB.
4. When you ask a question, the most relevant chunks are retrieved.
5. The context is passed to the Ollama-powered LLM, which generates a response.
6. The full interaction happens in a chat interface.

---

## ğŸ§° Tech Stack

- **Streamlit** â€” Web interface
- **ChromaDB** â€” Vector database
- **SentenceTransformers** â€” Text embedding
- **Ollama** â€” Local LLM inference
- **PyMuPDF** (`fitz`) â€” PDF text extraction

---
